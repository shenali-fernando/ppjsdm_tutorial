---
title: "PPJSDM Tutorial Part 2: Model Fitting"
format: html
editor: visual
---

# Tutorial Part 2: Model Fitting and Common Problems

At this point we should have decided on all parameter values and assigned them:

```{r}
configuration <- Configuration(data2$x_jitter, data2$y_jitter, data2$Species) 
plot(configuration)
window <- ppjsdm::Rectangle_window(x_range = c(0,320), 
                                   y_range = c(0,500))
model <- "bump"
ngroup <- length(levels(configuration$types)) #number of groups 
short_range <- matrix(3.75, ngroup, ngroup)
nthreads <- 4
saturation <- 10 
covariates <- list(elevation = elevation)
```

## Model Fit

To fit the model we run:

```{r}
set.seed(2) #set seed to get same results each time 

fit <- ppjsdm::gibbsm(configuration, 
                      window = window, 
                      model = model,
                      short_range = short_range,
                      fitting_package = "glmnet",
                      nthreads = nthreads,
                      saturation = saturation,
                      covariates = covariates,
                      dummy_distribution = "stratified",
                      min_dummy = 1, dummy_factor = 1e10, 
                      max_dummy = 1e3)

```

The number of dummy points given strongly influences the time taken to run the fit. When running initial models, we recommend lowering the max dummy point number for a quicker result. When running a final model, we recommend increasing this (around 1e4).

The fit of the model produces the alpha (interaction) and beta (environmental) coefficient matrix:

```{r}
coef(fit)
```

Here we can get the beta0, alpha and beta values as well as the parameterised values for short-range radius.

## Model Summary

This only gives the coefficients however. To get the confidence intervals, standard error and z-values for the coefficients, we can run the summary. The summary will take much more time than the fit of the model to run. Therefore we can set a time limit on how long the summary can run for.

```{r}
sum <- summary(fit, time_limit = 0.5)
sum
```

In the summary the first set of rows are the beta_0 for each group in the model. The beta0 coefficients for each group, when multiplied by the covariates, give the log-intensity (density) of each group if there were no interactions occurring between individuals. The alpha (alpha1) coefficients, or interaction coefficients, is produced for each pair of groups included in the model for the given short-range radii value. The magnitude and direction (negative or positive) of the interaction coefficient gives us the effect on the likelihood of finding two individuals of the two groups close to each other for the short-range interaction radii values. Therefore, an interaction coefficient of -1.5 would mean the log-likelihood of finding one individual of one group close to another individual of another group decreases by 1.5 when individuals are at the given interaction distance (R).

The next set of rows are for the environmental association coefficients and are given by the environmental covariate name (elevation, aridity, TWI, fire, u_density) to each group. These give the association of each group to a covariate. Therefore if there is an environmental coefficient of +0.5 would mean the likelihood of finding an individual of that group increases by 0.5 with every unit increase in the environmental covariate.

The columns are firstly the coefficient value and then the standard error of that as well as the high and low 95% confidence intervals. A significance score of one to three asterisks are given based on the CIs. P and Z values are also given. The last column is the se_numerical_proportion column which gives the percentage of standard error that is due to numerical uncertainty. Numerical uncertainty is due to the distribution and number of dummy points. Please refer to part 1 of this tutorial for an explanation of any of these parameters.

## Some Common Problems

At this point, you make happen across an error or warning when trying to fit the model or produce a summary of the model. I have noted some of these and their solutions below. If an issue that is not noted below occurs please create an issue on this Github repo ([https://github.com/shenali-fernando/ppjsdm_tutorial](https://github.com/shenali-fernando/ppjsdm_tutorial/tree/main)) or the package Github (<https://github.com/iflint1/ppjsdm>), or contact directly through email (shenalidfernando\@gmail.com, shenali.fernando.1\@unimelb.edu.au).

### Misspecification of Parameters

If parameters are misspecified in the fit you will see this warning message, and the interaction coefficient (alpha) matrix will be NULL.

```         
Warning message: from glmnet C++ code (error code -100); Convergence for 100th lambda value not reached after maxit=100000 iterations; solutions for larger lambdas returned  > coef(fit$alpha[[1]]) NULL
```

In this case, make sure that each group included in the model has a reasonable number of individuals and are not clustered in one area. Make sure that your set configuration matches with all other parameters specified. Lastly, try changing the parameters of model, short_range and saturation to get a balance that works for the point pattern in your data.

### Fitting Package

There is a parameter of fitting package, which identifies the fitting package of the logistic regression. The package 'glm' is the main fitting package. It is the standard way to fit a logistic regression. However, this package often fails when there are more dummy points than presences. However, if the glm package works for your dataset it is highly recommended to use that package.

The 'glmnet' package, importantly, uses shrinkage to force some coefficients to 0. In our model, we force the glmnet package to do a standard regression without the shrinkage. This gives it more stability. If the fit is not running, check this parameter is set to 'glmnet' and not 'glm'.

### Missing Values in Covariates

When there are missing values in covariates the model will not be able to run and this error message will appear:

```         
Error in (function (x, y, family = c("gaussian", "binomial", "poisson",  :    x has missing values; consider using makeX() to impute them In addition: Warning message: In fit_gibbs(gibbsm_data_list, fitting_package = fitting_package,  :   Some NA values detected in regression matrix; indices: 7514759876007608761176127617761876377712
```

In these cases, check the covariates have a value for each pixel cell.

### Binary covariates

As mentioned previously, the model does not work well if you have binary covariates. It is possible to smooth covariates so they are no longer binary. However, if you must use binary covariates in the model, call parameter [use_regularisation = TRUE]{.underline} in the fit of the model.

This allows the glmnet package to use some shrinkage, putting some coefficients to 0. As there is quasi-regularity in the fit now, when binary covariates have cells either 0 or 1, the model is now able to distinguish where a 0 might occur.

```{r}
use_regularisation = TRUE 
```

### Duplicate Points Warning

If there are points with the same coordinates in your data (duplicate points), you will get a warning when attempting to fit the package. This may cause problems down the track, so it is better to resolve it at this point. The best way to do this is apply a jitter to your points (all points or just duplicated points), with an amount less than the amount of the error in coordinate sampling. Eg. If your error in sampling coordinates was 5, give duplicated points a jitter of 2.5 so that these are all moved randomly 2.5 units.

### Numerical Error and Dummy Points

The numerical uncertainty (se_numerical_proportion) is the portion of the standard error on the estimates that is due to the dummy points. It is the It tends to zero as you increase the number of dummy points. Theoretically, the way this works is that the variance-covariance matrix of error can be split into two \\Sigma = \\Sigma_1 + \\Sigma_2, where \\Sigma_1 depends only on the dummy distribution and \\Sigma_2 depends on the fitted parameter \\theta. The numerical uncertainty is computed by using \\Sigma_1.Â 

So, increasing the dummy points can reduce the numerical uncertainty, however if the number of dummy points are increased too much, then the numerical methods break down. Remember, the set number of dummy points is for each type in the model. Generally, a max dummy point of 10e4 or 10e5 will reduce numerical uncertainty well, and many more dummy points than this will start to impact the regression methods.

### Bad_alloc

A bad_alloc error can appear when memory usage is too high. If possible, drop the number of dummy points or reduce the number of groups. Otherwise, you can use high powered computing software that allows for high memory usage.

### NAs in Summary

NAs occur when there are too many species and too many individuals. As the computation of the sigma matrix of the CIs is not possible due to time limits we approximate it in the model. To do this, we do not consider all of the points but consider a sub-window of the whole window. On the sub-window, the sigma is computed. This is done on multiple sub-windows, and then the average sigma matrix is taken. However, if we have non-homogenous groups, a sub-window may have no individuals and therefore the sigma matrix gives an NA resulting in an overall NA. This is why NAs occur in groups that have very few individuals or an extremely range-restricted group.

Most often in these cases, it is due to a time_limit parameter being given. Do not give this parameter, and let the summary run its whole time and this increases the draws of the sub-windows. We can also increase the number of sub-windows in the summary. If this does not solve the error, think about about combining or removing groups.

### Debug = TRUE

If you are having problems running the summary, add parameter debug = TRUE to the summary function. This will show at which part of the summary the error is occurring.\

### System Exactly Singular Error

When running the summary this error can occur:

```         
Error in solve.default(S) : 
  Lapack routine dgesv: system is exactly singular: U[15,15] = 0
```

This is occurring as we try to compute the variance-covariance matrix from the fitted coefficients. This error occurs as the fitted coefficients are a matrix of coefficients where all values are the same (most often all values are 0). To fix this issue, change the specification of the short-range interaction radii fitted. Make sure to check the specification for the medium- or long-range interaction radii if fitted. In particular, run an AIC to get the short-range radii into a range where the fit might work.
