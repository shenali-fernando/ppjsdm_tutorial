---
title: "PPJSDM Tutorial Part 1: Parameterisation"
format: html
editor: visual
---

# Tutorial Part 1: Model Parameterisation and Specification

## Purpose of Tutorial

This series of qmd documents is designed to be a step-by-step tutorial in using the saturated pairwise interaction Gibbs point process model. The model will be used with a dataset of tree locations from the Starvation Creek forest plot in Victoria, Australia. The aim of the tutorial is to show how parameterisation and fitting of the model can be done, and then how to visualise results and use the model for prediction. In providing an in-depth tutorial, we hope that the model and this workflow can be taken up and applied to other datasets.

## Install Package

Before installing the ppjsdm package. If you're using Windows please make sure RTools is downloaded (<https://cran.r-project.org/bin/windows/Rtools/>). This is not needed for Linux and Mac.

```{r}
install.packages("devtools")
library(devtools)

install_github("iflint1/ppjsdm")
library(ppjsdm)

plot(rppp())
```

The package should take a minute or two to download. If all is well, a plot of a point pattern should show.

We can also load the other libraries we need at this point.

```{r}
library(ggplot2)
library(dplyr)
```

## Load Data

```{r}

```

If you are using forestGEO data, you can use the load_forestGeo function from the 'helper_functions.R' script. This will load the data from your file path, clean data by removing NA rows and adding a jitter to the data, check the range of the data, and had other useful parameters to remove dead trees and saplings.

## Load Environmental Data

```{r}

```

## Parameters

There are a range of parameters supplied to the model. I will go through each parameters and recommendations on how to set these.

### Configuration

The configuration gives the individuals that we want to model. To supply the configuration we must give the x and y coordinates of each individual, as well as the type of individual. The type refers to the what group the individual belongs to. The model works by maximising the pseudo-likelihood of a group of individuals interacting with another group. This group can be species, genus, size class, or any other kind of grouping. For simplicity, we can specify our types as species here.

```{r}
configuration <- Configuration(data$x, data$y, types = data$Species)
plot(configuration)
```

### Window

The window gives the area the observed individuals exist in, and what (x, y) coordinates correspond into. In our dataset, the forest plot was 400m in length and 400m in width therefore we can set the window as such using the ppjsdm function. This can also be specified using spatstat im or owin objects.

```{r}
window <- ppjsdm::Rectangle_window(c(0,400), c(0,400))
```

### Short-range interaction radii (short-range) and potential shape (model)

The short-range interaction radius values and potential shape are described by the short-range and model parameter respectiively. These are the more complex parameters, and are important, as all outputs of the model are held to the set short-range radii. The short-range interaction radii are the typical distances from an individual where the likelihood of finding other individuals is affected. Therefore, at the set radii the magnitude of effect of an individual on the focal individual is halved. The shape of interaction potential expresses how the likelihood of finding another individuals changes with distance. We can have a look at the different interaction potential shapes we can set:\

```{r}
#Exponential
curve(exp(-x * log(2) / R), 0, 2 * R_2, ylab = "", xlab = "", ylim = c(0, 1), col = "#3B99B1",lwd = 3)

#Square exponential
curve(exp(-x * x * log(2) / (R * R)), 0, 2 * R_2, col = "#7CBA70", lwd = 3, add = TRUE) 

#Bump 
curve(1 - exp(-R * R * log(2) / (x * x)), 0, 2 * R_2, col = "#F5191C", lwd = 3, add = TRUE)

#Sqaure bump
curve(1 - exp(-R_1 * R_1 * log(2) / (x * x)), 0, 2 * R_2, col = "#E78F0A", lwd = 3, add = TRUE)

#Geyer/step
curve(ifelse(x >= 0 & x <= R, 1, 0), 0, 2 * R_2, col = "#111111", lwd = 3, add = TRUE)

```

The blue line gives the exponential shape, the green line the square-exponential, the red the bump, the orange the square-bump and lastly the black line gives the Geyer or step function.

Each shape shows that the affect of another individual. This may decreases smoothly in exponential or sharply as in square exponential with distance.

The parameterisation of both short-range parameters is done using AIC minimisation. This holds all other groups at a constant short-range radius, while the focal group is minimised over the a range of set short-range radii and the shapes of the potential.

We can create AIC minimisation graphs for each group in our data:

```{r}
plotlist <- list()

for (h in levels(configuration$types)) {
  to_optimize <- function(df) { #defines a new function that optimises each row in the dataframe df
  sapply(seq_len(nrow(df)), function(i) { #loops from 1 to number of rows in df, i in function is the index to iterate through the rows of the dataframe 
    set.seed(1)
    fit <- ppjsdm::gibbsm(configuration[h], #create the fit
                          window = window, 
                          model = df$model[i],
                          short_range = matrix(df$short[i]),
                          saturation = saturation,
                          dummy_distribution = "stratified",
                          min_dummy = 1, max_dummy =1e3,
                          dummy_factor = 1e10,
                          nthreads = 4, 
                          fitting_package = "glmnet")
                          
    fit$aic #take the aic value from the fit
  })
}

possible_short <- seq(from = 1, to = 15, length.out = 50) #possible short_range values
possible_model <- c("square_exponential", "exponential", "square_bump", "bump") #possible models
df <- expand.grid(short = possible_short, model = possible_model) #creating dataframe with possible values
df$aic <- to_optimize(df) #optimisation, run the created dataframe through the function
df$potentials <- df$model

plot <- ggplot(df) + geom_point(aes(x = short, y = aic, colour = potentials)) + ggtitle(h)

plotlist[[h]] <- plot
}

for (p in plotlist) {
  print(p)
}
```

Based on these graphs we can set the short-range radii values.

To set the same short-range radii value for each group included in the model:\

```{r}
ngroup <- length(levels(configuration$types)) #number of groups included in the model 
short_range <- matrix(5, ngroup, ngroup) #fill a matrix of ngroup number of rows and columns with 5
```

Looking at our AIC graphs, each group in the model has a slightly different short-range radii. We can set different short-range radii for each group in the model:

set off-diagonals - take the mean here, but can take max/other ways

```{r}
levels(configuration$types) #check groups in configuration 
diag <- c(10, 6, 4, 8, 2)  #set diagonal of matrix according to AIC results
n <- length(diag) #size of matrix 

short_range <- matrix(diag, n, n, byrow = TRUE) #make the matrix filled with diagonal values

for (i in 1:n) { 
  for (j in 1:n) {
    short_range[i, j] <- mean(diag[c(i, j)])#the function to fill each cell 
  }
}

short_range
```

We can set the shape of the potential as well. There can only be one potential model set for a model fit (no theoretical reason - can make one potential per type)

```{r}
model <- "exponential"
```

This model also allows for medium and long range interactions. When these are not specified they default to 0. I will not be using these parameters in this following example.

### Saturation

The saturation parameter describes the maximum number of individuals a focal individual can interact with in its interaction radius. This is in line with Rajala et al., (2018) who say as resources are finite the neighbour must saturate. Therefore, we can choose a value between 10 and 20. If this is not specified, the saturation defaults to 2. To validate this choice, we can fit iterative models changing the saturation parameter, and choose a saturation parameter value dependent on what maximises the pseudo-likelihood.

no theoretical reason to have one value

```{r}
saturation <- 10 
```

### Covariates

Environmental covariates must be an 'im' object to be applied to this model. If you have an older version of R, the package maptools provides functions to convert rasters and spatrasters into im form. However, in more recent versions of R, we can use the as.im.SpatRaster1 function from 'helper_functions. R', as following:

```{r}
as.im.SpatRaster1()
```

In addition to this function in the 'helper_functions. R' script, there are some other functions to convert covariates to im form. If your covariate is a matrix use function 'matrix_to_im'. If your covariate is in a rdata, asc or tif file, you can use function to_im to convert the covariate to an im object from the file path. Make sure to check your covariate is the correct range, ctrs projection and orientation for your window.

It is also useful to make sure your covariates vary by around the same amount. This is to make sure that an increase or decrease in environmental or beta coefficient value means the same in all covariates, and facilitates interpretation. In this case, I will make sure that all covariates vary by around one unit. This means I will divide the elevation covariate by 100, and take the log of the TWI or topographic wetness index covariate.

We then supply the covariates as a list of im objects.

```{r}
covariates <- list(elevation = elevation_im/100, TWI = TWI_im, aridity = aridity_im, u_density = u_density_im, fire = fire_im)

```

The last important note to make about covariates is that binary covariates do not work well in this model. Binary covariates are often used in ecology where we have areas where something appeared and areas where it didn't. This could be layers showing fire, land-use, roads or streams. In these cases, we can use functions like spatstat.explore::Smooth to allow the covariate to vary continuously and not as a binary.

### Dummy distribution

There are four parameters relating to the dummy distribution: dummy_distribution, min_dummy = 1, dummy_factor, and max_dummy.

We can think of dummy points acting similarly to background points in models such as Maxent. From a purely mathematical point of view, the dummy points are just an ad-hoc point process that allows us to compute an approximation of the likelihood which otherwise does not have a close form. Here is an analogy: in maths, when you want to approximate the value of a complex integral, you draw some random points, compute the function to integrate at those points, and deduce an approximation of the integral. The dummy points play the same kind of role here. In the original paper (Flint et al., 2022), they talk about dummy points in exactly this way: <https://people.math.aau.dk/~rw/Papers/lrlSPP.pdf>

From a more practical point of view, the dummy points can be interpreted as follows. In the GLM framework, we typically work with Gaussian, logistic, Poisson, etc, regressions. How can we fit a point process, which only contains point locations, with this framework? Well, we can add some randomly distributed dummy points, call these "absences" and then prove that the logistic regression (which models presence/absence) which uses those dummy points as absences gives an \*approximate\* fit of the point process, and the approximation gets better as we increase the number of dummy points. In this sense, you can call the dummy points "pseudo-absences", since they correspond to absences in an analogue logistic regression which approximates the true point process likelihood.

Dummy points allow the point process model (presence-only) to be approximated by a logistic regression (presence/absence) which is much easier to solve.

Parameters for the dummy points are specified in the fit. Dummy points can take a certain distribution through the region included in the model of binomial or stratified. A 'binomial' distribution draws each point uniformly (i.e with same probability of landing in any given location) over the window. A 'stratified' distribution divides the window into n equally sized boxes and draws exactly one point in each of the boxes. While both distributions result in the same number of dummy points, the stratified points are more regularly distributed. However, when the window is oddly shaped such as Australia-shaped, stratified tends to bug, so fall back to a binomial distribution.

The number of dummy points we set is for each group in the model, not an overall number. So, if we set 10 dummy points, that is 10 dummy points per group included in the model. The min_dummy gives the minimum dummy points per group. The dummy_factor is the factor the maximum dummy points increase by. Therefore, if we set the dummy points as below, we would have 100 points per group in the model. The larger the number of dummy points used the more closely approximately the point process is, and so the lower the se_numerical_proportion should be. However, this value can also be affected by how well the model fits the data.

```{r}
dummy_distribution <- "stratified"
min_dummy <- 1
dummy_factor <- 1e10
max_dummy <- 1e2
```

### nthreads 

nthreads specifies the number of cores of the CPU R uses. This is defaulted to 1. However, most computers/laptops have a more capacity that this. We can increase nthreads to allow for more computational power.

```{r}
nthreads <- 4
```

### return_raw_fitting_data 

This parameter is defaulted to FALSE. However if it is set to TRUE, it returns the raw fitting data before the glm has been done to produce standard errors, confidence intervals, etc. If you want to use your own glm fitting package to do this, you can extract the raw fitting data using the above parameter and give your package the coefficient matrix.
