---
title: "PPJSDM Tutorial Part 1: Parameterisation"
format: html
editor: visual
---

# Tutorial Part 1: Model Parameterisation and Specification

## Purpose of Tutorial

This series of qmd documents is designed to be a step-by-step tutorial in using the saturated pairwise interaction Gibbs point process model. This model (ppjsdm) is able to jointly model multiple-species so to understand and disentangle how tree arrangement in space is driven by interactions between neighbouring trees and environmental covariates.

The aim of the tutorial is to show how parameterisation and fitting of the model can be done, and then how to visualise results and use the model for prediction. In providing an in-depth tutorial, we hope that the model and this workflow can be taken up and applied to other datasets and help with troubleshooting issues that may come up.

## Install Package

To get started we need to install the ppjsdm package. Ppjsdm needs a C++ complier to work. Therefore, Windows users must download RTools at: <https://cran.r-project.org/bin/windows/Rtools>. If RTools is downloaded make sure the version of RTools is compatible with your version of R (eg. RTools 4.5 = R 4.5).

Mac users should make sure that xcode and a Fortran complier is downloaded. Please refer to the following guides: <https://mac.r-project.org/tools/> for xcode and gfortran, or <https://cran.r-project.org/bin/macosx/tools/> for gfortran downloading help. For recent versions of Mac and R the 'gfortran-12.2-universal.pkg' complier should solve issues resulting in 'library 'gfortran' not found' clang+++ error when downloading the package. For other versions of Mac and R, check version compatibility and download the compatible software from <https://cran.r-project.org/bin/macosx/tools/> or look to <https://www.cynkra.com/blog/2021-03-16-gfortran-macos/> for more help.

If download errors are still occurring make sure package Rcpp is downloaded and updated as well as other Rcpp packages (RcppArmadillo, RcppEigen, RcppProgress, RcppThread).

```{r}
install.packages("devtools")
library(devtools)

install_github("iflint1/ppjsdm")
library(ppjsdm)

plot(rppp())
```

The package should take a minute or two to download. If all is well, a plot of a point pattern should show.

We can also load the other libraries we need at this point.

```{r}
library(ggplot2)
library(dplyr)
library(spatstat)
library(spatstat.geom)
```

There are also some helper functions that may be useful at some point, stored in the script 'helper_functions.R'. This script is available in the same repo as this Rmd. We can source this file now:

```{r}
source('helper_functions.R')
```

## Load Data

The dataset we will use in this tutorial is from the Luquillo dataset from the Smithsonian Institution's ForestGEO Network. The [Luquillo Experimental Forest Plot](https://forestgeo.si.edu/sites/north-america/luquillo) is a long-term research site located in the tropical rainforest of Puerto Rico and was established in 1992. The Luquillo plot is 16-ha with dimensions 320 x 500 metres.

The Luquillo dataset is freely available through the fgeo.data package. We can load in the data:

```{r}
#devtools::install_github("forestgeo/fgeo.data")
library(fgeo.data)
data <- luquillo_stem_random
head(data)
```

If you are using forestGEO data, you can use the load_forestGeo function from the 'helper_functions.R' script. This will load the data from your file path, clean data by removing NA rows and adding a jitter to the data, check the range of the data, and had other useful parameters to remove dead trees and saplings. This will also load the covariates.

We can do this manually as well for some practice!

The most important columns to note are the CensusID and Status. For our purposes, we want the data from a single census. We also only want the trees marked alive and want to get rid of any NA or dead trees.

```{r}
data <- data %>% 
  filter(CensusID == 6) %>% 
  filter(DFstatus == "alive", na.rm = TRUE)

head(data)
```

Now we have the cleaned data. The data has the 'treeID' and 'stemID' that was given to each tree in the census. The 'dbh' and 'pom' columns give the diameter at breast height and the point of measure for each tree. The 'gx' and 'gy' columns give the global x and y coordinates for the trees in the plot. The 'sp' column gives the species of the tree. We can check what the species code refers to in the botanical name format as so:

```{r}
taxa <- luquillo_taxa
```

## Load Environmental Data

The fgeo.data package also provides the elevation data for the Luquillo plot:

```{r}
elevation <- luquillo_elevation
```

## Parameters

There are a range of parameters supplied to the model. I will go through each parameters and recommendations on how to set these.

### Configuration

The configuration gives the individuals that we want to model. To supply the configuration we must give the x and y coordinates of each individual, as well as the type of individual. The type refers to the what group the individual belongs to. The model works by maximising the pseudo-likelihood of a group of individuals interacting with another group. This group can be species, genus, size class, or any other kind of grouping. For simplicity, we can specify our types as species here.

```{r}
#| fig-height: 8
configuration <- Configuration(data$gx, data$gy, types = data$sp)
plot(configuration)
```

We have quite a few species, we might want to check if we have very rare species.

```{r}
data %>% count(sp)
```

We have quite a few species with less than 20 individuals. The model can run with around 10 individuals in a group but the uncertainty will be large. For simplicity and to get some certain coefficients I will choose to put species with less than 20 individuals into a miscellaneous class and also remove the miscellaneous class.

```{r}
data <- data %>% group_by(sp) %>% 
mutate(observation_count = n()) %>% 
  ungroup() %>% 
  mutate(Species = if_else(observation_count > 20, sp, "MISC")) %>% 
  filter(! Species == "MISC")

data %>% count(Species)
```

That's better! Let's make the configuration again:

```{r}
configuration <- Configuration(data$gx, data$gy, types = data$Species)
plot(configuration)
```

We're getting the warning message: There are duplicate points in the configuration, this may pose problems when trying to fit the model or run the summary. It is good practice to apply a jitter to make sure there are no duplicate points.

```{r}
data2 <- data %>%
  group_by(gx, gy) %>% #group by coordinate columns
  mutate(
    is_duplicated = n() > 1, #create column of TRUE/FALSE 
    #new_column_name = if_else(condition, true, false): so condition=column name, if true=fill with, if false=fill with
    x_jitter = if_else(is_duplicated, pmin(pmax(gx + runif(n(), -0.5, 0.5), 0), 320), gx), #create x_jitter column, pmax/pmin=sets max and min values
    y_jitter = if_else(is_duplicated, pmin(pmax(gy + runif(n(), -0.5, 0.5), 0), 500), gy) #create y_jitter column
  ) %>%
  ungroup() %>% 
  dplyr::select(-is_duplicated)


# We have a configuration 
configuration <- Configuration(data2$x_jitter, data2$y_jitter, data2$Species) 
plot(configuration)
```

### Window

The window gives the area the observed individuals exist in, and what (x, y) coordinates correspond into. In our dataset, the forest plot was 320m in the x and 500m in the y therefore we can set the window as such using the ppjsdm function.We can also check this using funcations like 'range'. This can also be specified using spatstat im or owin objects, and can be specified for non-rectangle windows.

```{r}
window <- ppjsdm::Rectangle_window(x_range = c(0,320), 
                                   y_range = c(0,500))
window
```

### Short-range interaction radii (short-range) and potential shape (model)

The short-range interaction radius values and potential shape are described by the short-range and model parameter respectively. These are the more complex parameters, and are important, as all outputs of the model are held to the set short-range radii. The short-range interaction radii are the typical distances from an individual where the likelihood of finding other individuals is affected. Therefore, at the set radii the magnitude of effect of an individual on the focal individual is halved. The shape of interaction potential expresses how the likelihood of finding another individuals changes with distance. We can have a look at the different interaction potential shapes we can set:

```{r}
R <- 1 
R_1 <- 1.5
R_2 <- 3 

#Exponential
curve(exp(-x * log(2) / R), 0, 2 * R_2, ylab = "", xlab = "", ylim = c(0, 1), col = "#3B99B1",lwd = 3)

#Square exponential
curve(exp(-x * x * log(2) / (R * R)), 0, 2 * R_2, col = "#7CBA70", lwd = 3, add = TRUE) 

#Bump 
curve(1 - exp(-R * R * log(2) / (x * x)), 0, 2 * R_2, col = "#F5191C", lwd = 3, add = TRUE)

#Sqaure bump
curve(1 - exp(-R_1 * R_1 * log(2) / (x * x)), 0, 2 * R_2, col = "#E78F0A", lwd = 3, add = TRUE)

#Geyer/step
curve(ifelse(x >= 0 & x <= R, 1, 0), 0, 2 * R_2, col = "#111111", lwd = 3, add = TRUE)

```

The blue line gives the exponential shape, the green line the square-exponential, the red the bump, the orange the square-bump and lastly the black line gives the Geyer or step function.

Each shape shows that the affect of another individual. This may decreases smoothly in exponential or sharply as in square exponential with distance.

The parameterisation of both short-range parameters is done using AIC minimisation. This holds all other groups at a constant short-range radius, while the focal group is minimised over the a range of set short-range radii and the shapes of the potential.

We can create AIC minimisation graphs for each group in our data. This takes about three to five minutes to run:

```{r}
#| warning: false
plotlist <- list()

for (h in levels(configuration$types)) {
  to_optimize <- function(df) { #defines a new function that optimises each row in the dataframe df
  sapply(seq_len(nrow(df)), function(i) { #loops from 1 to number of rows in df, i in function is the index to iterate through the rows of the dataframe 
    set.seed(1)
    fit <- ppjsdm::gibbsm(configuration[h], #create the fit
                          window = window, 
                          model = df$model[i],
                          short_range = matrix(df$short[i]),
                          saturation = 10,
                          dummy_distribution = "stratified",
                          min_dummy = 1, max_dummy =1e3,
                          dummy_factor = 1e10,
                          nthreads = 4, 
                          fitting_package = "glmnet")
                          
    fit$aic #take the aic value from the fit
  })
}

possible_short <- seq(from = 1, to = 20, length.out = 25) #possible short_range values
possible_model <- c("square_exponential", "exponential", "square_bump", "bump") #possible models
df <- expand.grid(short = possible_short, model = possible_model) #creating dataframe with possible values
df$aic <- to_optimize(df) #optimisation, run the created dataframe through the function
df$potentials <- df$model

 plot <- ggplot(df) + geom_point(aes(x = short, y = aic, colour = potentials)) + ggtitle(h)

plotlist[[h]] <- plot
}

for (p in plotlist) {
  print(p)
}
```

Based on these graphs we can set the short-range radii values. Only two species have produced AIC graphs that give a result, these being PREMON and CECSCH. Sometimes, running the AIC will give a warning that lambda has not converged - this means that something has been misspecified in the model. WE can see this where the some models for some values of short-range do not show an AIC score.

We can see that for PREMON the AIC reaches a minimum in the exponential model at 3.5. For CECSCH a minimum is reached at very small or very large values. In cases like these where the AIC scores are unable to give a optimal short-range value and model for most species we can do some experimentation with these parameters and see which specification allows a robust model. We can also aim to choose a model and radius where there is minimal change in the AIC scores so to reduce large variation in the model ouputs. When experimenting with different short-range parameters, these can be chosen based on a number of criteria (eg. number of significant coefficients, number of large coefficients, the models aic score shown in fit\$aic, number of unreasonable coefficients, etc). However, we do know that our model is **not sensitive** to misspecifications of the short-range parameters.

In this case, as all species had a full AIC score in the bump model I will choose this model and a short-range radius of 3.75. We can see that for species PREMON the AIC is minimised when the the bump model is around 3.75. All other species have a very flat bump model at 3.75 so it doesn't seem to be a bad radius to choose.

To set the same short-range radii value for each group included in the model:

```{r}
ngroup <- length(levels(configuration$types)) #number of groups included in the model 
short_range <- matrix(3.75, ngroup, ngroup) #fill a matrix of ngroup number of rows and columns with 5
```

If we think that each group in the model has a different short-range radii. We can set different short-range radii for each group in the model as such:

We set the diagonals (interaction of individuals of the same group) as determined by AIC or another measure, and then we use the mean of the diagonals to set the off-diagonals. The off-diagonals could also be set by the maximum, minimum or median of the two diagonals involved.

```{r}
#| eval: false 

levels(configuration$types) #check groups in configuration 
diag <- seq(1:8)  #set diagonal of matrix where group 1 has a short-range of 1 
n <- length(diag) #size of matrix 

short_range <- matrix(diag, n, n, byrow = TRUE) #make the matrix filled with diagonal values

for (i in 1:n) { 
  for (j in 1:n) {
    short_range[i, j] <- mean(diag[c(i, j)])#the function to fill each cell 
  }
}

short_range
```

We can set the shape of the potential as well. There can only be one potential model set for a model fit (no theoretical reason - can make one potential per type).

```{r}
model <- "bump"
```

This model also allows for medium and long range interactions. When these are not specified they default to 0. I will not be discussing these parameters in this following example.

### Saturation

The saturation parameter describes the maximum number of individuals a focal individual can interact with in its interaction radius. This is in line with Rajala et al., (2018) who say as resources are finite the neighbour must saturate. Therefore, we can choose a value between 10 and 20. If this is not specified, the saturation defaults to 2. To validate this choice, we can fit iterative models changing the saturation parameter, and choose a saturation parameter value dependent on what maximises the pseudo-likelihood. Again, there is no theoretical reason to have one value and we can adjust the model so that different types have different saturations.

```{r}
saturation <- 10 
```

### Covariates

Environmental covariates must be an 'im' object to be applied to this model. If you have an older version of R, the package maptools provides functions to convert rasters and spatrasters into im form. However, in more recent versions of R, we can use the as.im.SpatRaster1 function from 'helper_functions. R'.In addition to this function in the 'helper_functions. R' script, there are some other functions to convert covariates to im form. If your covariate is a matrix use function 'matrix_to_im'. If your covariate is in raster, spatraster or rasterlayer form use make_covariates to convert the file path to im object. If your covariate is in a rdata, asc or tif file, you can use function to_im to convert the covariate to an im object from the file path. Make sure to check your covariate is the correct range, ctrs projection and orientation for your window.

In the Luquillo dataset, we have been provided the elevation dataset. Let's check it's class:

```{r}
class(elevation)
```

As it's a list, we can convert it to a 'im' object as so:

```{r}
xrange <- c(0, luquillo_elevation$xdim)
yrange <- c(0, luquillo_elevation$ydim)
matr <- as.matrix(luquillo_elevation$mat)
elevation <- spatstat.geom::im(matr, 
                           xrange = xrange, 
                           yrange = yrange)
plot(elevation)
```

If you have multiple covariates it is useful to make sure your covariates vary by around the same amount. This is to make sure that an increase or decrease in environmental or beta coefficient value means the same in all covariates, and facilitates interpretation.

We then supply the covariates as a list of im objects.

```{r}
covariates <- list(elevation = elevation)
```

Another important note to make about covariates is that binary covariates do not work well in this model. Binary covariates are often used in ecology where we have areas where something appeared and areas where it didn't. This could be layers showing fire, land-use, roads or streams. In these cases, we can use functions like spatstat.explore::Smooth to allow the covariate to vary continuously and not as a binary. Lastly, for general good practice and to reduce instability in the models we need to check for colinearity in the covariates. This can be done in any way you prefer.

### Dummy distribution

There are four parameters relating to the dummy distribution: dummy_distribution, min_dummy = 1, dummy_factor, and max_dummy.

We can think of dummy points acting similarly to background points in models such as Maxent. From a purely mathematical point of view, the dummy points are just an ad-hoc point process that allows us to compute an approximation of the likelihood which otherwise does not have a close form. Here is an analogy: in maths, when you want to approximate the value of a complex integral, you draw some random points, compute the function to integrate at those points, and deduce an approximation of the integral. The dummy points play the same kind of role here. In the original paper (Flint et al., 2022), they talk about dummy points in exactly this way:Â <https://people.math.aau.dk/~rw/Papers/lrlSPP.pdf>

From a more practical point of view, the dummy points can be interpreted as follows. In the GLM framework, we typically work with Gaussian, logistic, Poisson, etc, regressions. How can we fit a point process, which only contains point locations, with this framework? Well, we can add some randomly distributed dummy points, call these "absences" and then prove that the logistic regression (which models presence/absence) which uses those dummy points as absences gives an \*approximate\* fit of the point process, and the approximation gets better as we increase the number of dummy points. In this sense, you can call the dummy points "pseudo-absences", since they correspond to absences in an analogue logistic regression which approximates the true point process likelihood.

Dummy points allow the point process model (presence-only) to be approximated by a logistic regression (presence/absence) which is much easier to solve.

Parameters for the dummy points are specified in the fit. Dummy points can take a certain distribution through the region included in the model of binomial or stratified. A 'binomial' distribution draws each point uniformly (i.e with same probability of landing in any given location) over the window. A 'stratified' distribution divides the window into n equally sized boxes and draws exactly one point in each of the boxes. While both distributions result in the same number of dummy points, the stratified points are more regularly distributed. However, when the window is oddly shaped such as Australia-shaped, stratified tends to bug, so fall back to a binomial distribution.

The number of dummy points we set is for each group in the model, not an overall number. So, if we set 10 dummy points, that is 10 dummy points per group included in the model. The min_dummy gives the minimum dummy points per group. The dummy_factor is the factor the maximum dummy points increase by. Therefore, if we set the dummy points as below, we would have 100 points per group in the model. The larger the number of dummy points used the more closely approximately the point process is, and so the lower the se_numerical_proportion should be. However, this value can also be affected by how well the model fits the data.

```{r}
dummy_distribution <- "stratified"
min_dummy <- 1
dummy_factor <- 1e10
max_dummy <- 1e2
```

### nthreads

nthreads specifies the number of cores of the CPU R uses. This is defaulted to 1. However, most computers/laptops have a more capacity that this. We can increase nthreads to allow for more computational power.

```{r}
nthreads <- 4
```

### return_raw_fitting_data

This parameter is defaulted to FALSE. However if it is set to TRUE, it returns the raw fitting data before the glm has been done to produce standard errors, confidence intervals, etc. If you want to use your own glm fitting package to do this, you can extract the raw fitting data using the above parameter and give your package the coefficient matrix.
