---
title: "PPJSDM Tutorial Part 2: Model Fitting"
format: html
editor: visual
---

# Tutorial Part 2: Model Fitting and Common Problems

At this point we should have decided on all parameter values and assigned them:

```{r}
configuration <- Configuration(data$x, data$y, types = data$Species)
window <- ppjsdm::Rectangle_window(c(0,400), c(0,400))
model <- "exponential"
short_range <- short_range
nthreads <- 4
saturation <- 10 
covariates <- list(elevation = elevation_im/100, TWI = TWI_im, aridity = aridity_im, u_density = u_density_im, fire = fire_im)
```

## Model Fit

To fit the model we run:

```{r}
set.seed(2) #set seed to get same results each time 

fit <- ppjsdm::gibbsm(configuration, 
                      window = window, 
                      model = model,
                      short_range = short_range,
                      fitting_package = "glmnet",
                      nthreads = nthreads,
                      saturation = saturation,
                      covariates = covariates,
                      dummy_distribution = "stratified",
                      min_dummy = 1, dummy_factor = 1e10, 
                      max_dummy = 1e2)

```

The number of dummy points given strongly influences the time taken to run the fit. When running initial models, we recommend lowering the max dummy point number for a quicker result. When running a final model, we recommend increasing this (around 1e4).

The fit of the model produces the alpha (interaction) and beta (environmental) coefficient matrix:

```{r}
coef(fit)
```

## Model Summary

This only gives the coefficients however. To get the confidence intervals, standard error and z-values for the coefficients, we can run the summary. The summary will take much more time than the fit of the model to run. Therefore we can set a time limit on how long the summary can run for.

```{r}
sum <- summary(fit, time_limt = 0.5)
```

In the summary the first set of rows are the beta_0 for each group in the model. The beta0 coefficients for each group, when multiplied by the covariates, give the log-intensity (density) of each group if there were no interactions occurring between individuals. The alpha (alpha1) coefficients, or interaction coefficients, is produced for each pair of groups included in the model for the given short-range radii value. The magnitude and direction (negative or positive) of the interaction coefficient gives us the effect on the likelihood of finding two individuals of the two groups close to each other for the short-range interaction radii values. Therefore, an interaction coefficient of -1.5 would mean the log-likelihood of finding one individual of one group close to another individual of another group decreases by 1.5 when individuals are at the given interaction distance (R).

The next set of rows are for the environmental association coefficients and are given by the environmental covariate name (elevation, aridity, TWI, fire, u_density) to each group. These give the association of each group to a covariate. Therefore if there is an environmental coefficient of +0.5 would mean the likelihood of finding an individual of that group increases by 0.5 with every unit increase in the environmental covariate.

The columns are firstly the coefficient value and then the standard error of that as well as the high and low 95% confidence intervals. A significance score of one to three asterisks are given based on the CIs. P and Z values are also given. The last column is the se_numerical_proportion column which gives the percentage of standard error that is due to numerical uncertainty. Numerical uncertainty is due to the distribution and number of dummy points. Please refer to part 1 of this tutorial for an explanation of any of these parameters.

## Some Common Problems

At this point, you make happen across an error or warning when trying to fit the model or produce a summary of the model. I have noted some of these and their solutions below.

### Fitting Package

There is a parameter of fitting package, which identifies the fitting package of the logistic regression. The package 'glm' is the main fitting package. It is the standard way to fit a logistic regression. However, this package often fails when there are more dummy points than presences. However, if the glm package works for your dataset it is highly recommended to use that package.

The 'glmnet' package, importantly, uses shrinkage to force some coefficients to 0. In our model, we force the glmnet package to do a standard regression without the shrinkage. This gives it more stability. If the fit is not running, check this parameter is set to 'glmnet' and not 'glm'.

### Binary covariates

As mentioned previously, the model does not work well if you have binary covariates. It is possible to smooth covariates so they are no longer binary. However, if you must use binary covariates in the model, call parameter [use_regularisation = TRUE]{.underline} in the fit of the model.

This allows the glmnet package to use some shrinkage, putting some coefficients to 0. As there is quasi-regularity in the fit now, when binary covariates have cells either 0 or 1, the model is now able to distinguish where a 0 might occur.

```{r}
use_regularisation = TRUE 
```

### Error Message: Same number of points 

You may get this error message at the summary.

```         
Error: The dummy points and the independent draw of a stratified binomial point process should have the same number of points. This could be due to NA values on the covariates, causing some of the dummy points to be removed on the draws. To solve this, either subset the window to locations where the covariates are non-NA, or avoid dummy points distributed as a stratified point process.
```

When the model fits it takes one draw of the stratified dummy points. When the model computes the summary it takes another draw of the stratified dummy points. These two draws **must** have the same number of points and be independent of each other or else the computation of the summary cannot work. This is as we are looking at the distances between dummy points to understand error.

The stratified dummy distribution (default) splits the observation window into quadrants and simulates one dummy point per quadrant. If the covariate has an NA value where the dummy point is, the dummy points becomes an NA and is removed, therefore we have a different number of points in the two draws and the computation cannot happen.

Therefore, every cell in your covariates must be filled - there **cannot** be NA values. To fill in NA values in covariates, you can take a mean of the cells around the NA cell, extrapolation of the data, or sample the covariate again. Either way, this NA value **must** be filled.

However, this error can occur when there are no NA values in your covariate. In this case, you might be using a non-square or non-rectangular window and therefore must use the binomial dummy distribution instead.

### Duplicate Points Warning

If there are points with the same coordinates in your data (duplicate points), you will get a warning when attempting to fit the package. This may cause problems down the track, so it is better to resolve it at this point. The best way to do this is apply a jitter to your points (all points or just duplicated points), with an amount less than the amount of the error in coordinate sampling. Eg. If your error in sampling coordinates was 5, give duplicated points a jitter of 2.5 so that these are all moved randomly 2.5 units.

### Numerical Error and Dummy Points

The numerical uncertainty (se_numerical_proportion) is the portion of the standard error on the estimates that is due to the dummy points. It is the It tends to zero as you increase the number of dummy points. Theoretically, the way this works is that the variance-covariance matrix of error can be split into two \\Sigma = \\Sigma_1 + \\Sigma_2, where \\Sigma_1 depends only on the dummy distribution and \\Sigma_2 depends on the fitted parameter \\theta. The numerical uncertainty is computed by using \\Sigma_1.Â 

So, increasing the dummy points can reduce the numerical uncertainty, however if the number of dummy points are increased too much, then the numerical methods break down. Remember, the set number of dummy points is for each type in the model. Generally, a max dummy point of 10e4 or 10e5 will reduce numerical uncertainty well, and many more dummy points than this will start to impact the regression methods.

### Bad_alloc

A bad_alloc error can appear when memory usage is too high. If possible, drop the number of dummy points or reduce the number of groups. Otherwise, you can use high powered computing software that allows for high memory usage.

### NAs in Summary

NAs occur when there are too many species and too many individuals. As the computation of the sigma matrix of the CIs is not possible due to time limits we approximate it in the model. To do this, we do not consider all of the points but consider a sub-window of the whole window. On the sub-window, the sigma is computed. This is done on multiple sub-windows, and then the average sigma matrix is taken. However, if we have non-homogenous groups, a sub-window may have no individuals and therefore the sigma matrix gives an NA resulting in an overall NA. This is why NAs occur in groups that have very few individuals or an extremely range-restricted group.

Most often in these cases, it is due to a time_limit parameter being given. Do not give this parameter, and let the summary run its whole time and this increases the draws of the sub-windows. We can also increase the number of sub-windows in the summary. If this does not solve the error, think about about combining or removing groups.

### Debug = TRUE

If you are having problems running the summary, add parameter debug = TRUE to the summary function. This will show at which part of the summary the error is occurring.

consider points on edge of window then fit

### R Session Aborted

The R session crashes when the virtual memory is filled. This usually occurs when there has been a misspecification in any part of the modelling if have covered. Therefore, check for errors such as in specifying the types or coordinates in the configuration, and other parameter errors.

### 
